# Week 9 â€“ Split Strategy Showdown  
## Partner Comparison: Random Holdout vs Time-Aware Split  

This document summarizes and compares the evaluation strategies for:

- **Partner A â€“ Andrea Churchwell**  
  - Random 80/20 holdout  
  - 5-fold KFold cross-validation  

- **Partner B â€“ Jose Diaz**  
  - Ordered 80/20 holdout (first 80% â†’ train, last 20% â†’ test)  
  - 5-fold TimeSeriesSplit (time-aware CV)  

The goal is to:

1. Describe the dataset and model setup.  
2. Summarize Partner A and Partner B results.  
3. Compare cross-validation stability and residual errors.  
4. Explain which strategy is more appropriate for the diabetes dataset and why.  

---

### ğŸ” Executive Summary
- Partner A's Random Holdout + KFold produced more stable and higher average RÂ² scores.
- Partner B's TimeSeriesSplit had much higher variance due to the dataset not being temporal.
- Visuals confirm that Partner Aâ€™s model generalizes more consistently (residuals, scatterplots, CV).
- Recommendation: Use Partner Aâ€™s approach for this dataset; Partner Bâ€™s approach fits true time series.


## 1. Dataset Overview  

- **Name:** Diabetes Regression (`sklearn.datasets.load_diabetes`)  
- **Samples:** 442 patients  
- **Features:** 10 numeric health measurements (age, BMI, blood pressure, etc.)  
- **Task Type:** Regression â€“ predict a continuous disease progression score  
- **Metric:** RÂ²  

This is a **small, noisy medical dataset**, which means:

- Different train/test splits can give noticeably different scores.  
- RÂ² values around **0.45â€“0.55** are normal.  
- Stability (variance across splits) is just as important as the raw RÂ² number.  

---

## 2. Partner A â€“ Random Holdout + KFold (Andrea)  

### Strategy  

- **80/20 random train/test split** using `train_test_split(..., shuffle=True, random_state=42)`.  
- **5-fold KFold** cross-validation on the training data.  
- Model: `StandardScaler` + `Ridge` regression combined in a `Pipeline`.  
- Metric: RÂ².  

Using a `Pipeline` prevents data leakage because the scaler is fit **only on the training data** inside each split or fold.  

### Train/Test Performance  

- **Train RÂ²:** 0.5276  
- **Test RÂ²:** 0.4541  

Interpretation:  

- The model explains about **52â€“53%** of the variance on the training set.  
- It explains about **45%** of the variance on unseen test data.  
- The drop from train â†’ test is **small and normal** for a small, noisy dataset.  
- This suggests the model is learning real patterns and generalizes reasonably well.  

### 5-Fold CV (KFold) Results  

Fold RÂ² scores:

- Fold 1: 0.4699  
- Fold 2: 0.5381  
- Fold 3: 0.4133  
- Fold 4: 0.4903  
- Fold 5: 0.4922  

Summary:

- **Mean RÂ²:** ~0.4808  
- **Std (variance):** ~0.04  

This shows **low-to-moderate variance**: the model behaves similarly across folds.  
For this dataset, Partner Aâ€™s strategy gives **stable** and **reliable** estimates.  

There is some wobble between folds (e.g., Fold 2 higher, Fold 3 lower), which is expected with only 442 samples.  

---

## 3. Partner B â€“ Ordered Holdout + TimeSeriesSplit (Jose)  

### Strategy  

- **Ordered 80/20 split**:  
  - First 80% of rows â†’ training set  
  - Last 20% of rows â†’ test set  
- **5-fold TimeSeriesSplit** on the training data (no shuffling; later folds always validate on â€œlaterâ€ rows).  
- Same model as Partner A: `StandardScaler` + `Ridge` in a `Pipeline`.  
- Metric: RÂ².  

This simulates a **time-aware** evaluation: train on â€œearlierâ€ data, validate on â€œlaterâ€ data, even though the diabetes dataset is not truly time series.  

### Train/Test Performance  

- **Train RÂ²:** 0.5088  
- **Test RÂ²:** 0.5413  

Interpretation:  

- On this particular ordered split, the last 20% of the data (test set) is slightly **easier** than the earlier rows.  
- Thatâ€™s why test RÂ² ends up a bit higher than train RÂ².  
- This is not an error; it can happen on small datasets with non-random splits.  

### 5-Fold TimeSeriesSplit Results  

Fold RÂ² scores:

- Fold 1: 0.2984  
- Fold 2: 0.5760  
- Fold 3: 0.2785  
- Fold 4: 0.5831  
- Fold 5: 0.4492  

Summary:

- **Mean RÂ²:** ~0.4373  
- **Std (variance):** ~0.13  

Compared to Partner A, Partner Bâ€™s strategy has:

- A **lower mean RÂ²**, and  
- A **much higher variance** across folds.  

TimeSeriesSplit forces each fold to train on earlier rows and validate on later rows.  
For a dataset that is **not actually temporal**, this mostly increases instability instead of adding realism.  

---

## 4. Cross-Validation Summary  

From `comparison.csv` (combined Partner A + Partner B CV scores):  

| Strategy    | Mean RÂ² | Std (variance) |
|------------|--------:|---------------:|
| Partner A  | ~0.48   | ~0.04          |
| Partner B  | ~0.44   | ~0.13          |

Key points:

- Partner A has a **higher average RÂ²** and **much lower variance**.  
- Partner B has a **lower average RÂ²** and **very bouncy scores** (from ~0.28 to ~0.58).  

This matches what you see in the CV bar charts:  

- `assets/partner_a_cvr2.png`  
- `assets/partner_b_cvr2.png`  

Open these two images side by side in VS Code to visually confirm the difference.  

---

## 5. Visual Comparisons
Below are our key visuals and interpretations for both strategies.

Below are the screenshots for both partners, embedded directly into the document.

## 5.1 Cross-Validation Bar Charts (Side-by-Side)
### What Weâ€™re Looking At (Cross-Validation)

- **Each bar = one fold of cross-validation** (5 total).
- Higher bars = better model performance.
- Differences in bar height = how â€œstableâ€ the model is across different slices of data.

**Partner A (Random KFold):**
- Bars are grouped tightly together.
- Scores range from ~0.41 to ~0.54.
- Very small variance (std â‰ˆ 0.04).
- This means **the model behaves consistently** no matter how the data is split.

**Partner B (TimeSeriesSplit):**
- Bars swing dramatically between folds.
- Scores range from ~0.27 to ~0.58.
- Very large variance (std â‰ˆ 0.13).
- This means the model is **unstable** under the time-aware split (because the dataset is NOT temporal).

**Interpretation:**
- Partner Aâ€™s strategy is more reliable and predictable.
- Partner Bâ€™s strategy introduces unnecessary instability for this dataset.
<table>
<tr>
<td>
<strong>Partner A â€“ 5-Fold CV RÂ²</strong><br>
<img src="assets/partner_a_cvr2.png" width="430">
</td>

<td>
<strong>Partner B â€“ 5-Fold CV RÂ²</strong><br>
<img src="assets/partner_b_cvr2.png" width="430">
</td>
</tr>
</table>
**Summary:** Partner Aâ€™s folds show tight, consistent performance, while Partner Bâ€™s folds swing dramatically, proving that KFold gives much more stable estimates for this dataset.

---
## 5.2 Actual vs Predicted
### What Weâ€™re Looking At (Actual vs Predicted)

These scatterplots show:
- **X-axis = actual disease progression**
- **Y-axis = modelâ€™s predicted score**

A perfect model would have all points lying exactly on a diagonal â€œmatchâ€ line.

**Partner A:**
- Upward trend is more visible.
- Points cluster more tightly around the dashed trendline.
- This indicates the model is capturing the linear relationship reasonably well.
- Still some noise â€” expected for this small dataset.

**Partner B:**
- The upward trend is still there but the spread of points is wider.
- Without the trendline, it's clear the relationship is noisier.
- This reflects the higher variance observed in cross-validation.

**Interpretation:**
- Both models learn the general pattern.
- Partner A has slightly better alignment with actual values.
- Partner B displays more scatter (less consistent predictions).

<table>
<tr>
<td>
<strong>Partner A â€“ Actual vs Predicted</strong><br>
<img src="assets/partner_a_actual_vs_pred.png" width="430">
</td>

<td>
<strong>Partner B â€“ Actual vs Predicted</strong><br>
<img src="assets/partner_b_actual_vs_pred.png" width="430">
</td>
</tr>
</table>
**Summary:** Both partners capture the general trend, but Partner Aâ€™s predictions align more closely with actual values, showing a cleaner and more reliable linear relationship.

---

## 5.3 Residual Histograms
### What Weâ€™re Looking At (Residuals)

Residual = (Predicted âˆ’ Actual).  
A good model has residuals clustered around **0**, meaning predictions are close to reality.

**Partner A:**
- Residuals are more tightly centered.
- Histogram shape is smoother.
- Fewer extreme errors.
- This matches the lower variance from the CV results.

**Partner B:**
- Residuals are more spread out.
- More extreme prediction errors.
- This reflects instability caused by the time-aware split.

**Interpretation:**
- Partner Aâ€™s model makes more consistent errors.
- Partner Bâ€™s model has wider, less predictable error patterns.
- Again, this supports the conclusion that KFold is the more appropriate strategy.

<table>
<tr>
<td>
<strong>Partner A â€“ Residual Histogram</strong><br>
<img src="assets/partner_a_residuals.png" width="430">
</td>

<td>
<strong>Partner B â€“ Residual Histogram</strong><br>
<img src="assets/partner_b_residuals.png" width="430">
</td>
</tr>
</table>
**Summary:** Partner Aâ€™s residuals cluster tightly around zero, while Partner Bâ€™s are more spread out, confirming that the time-aware split introduces greater prediction error.

---

## 6. Recommendation  

Both partners correctly implemented their assigned strategies:

- Partner A: random 80/20 split + 5-fold KFold CV.  
- Partner B: ordered 80/20 split + 5-fold TimeSeriesSplit (time-aware CV).  

However, the **diabetes dataset is not temporal** â€” the row order does not represent time.

Because of that:

- Randomized KFold (Partner A) is **better suited** to this dataset.  
- It produces **higher mean RÂ²** and **much more stable** results.  
- TimeSeriesSplit (Partner B) mainly increases variance without adding realism here.  

**Final conclusion:**  

> For the Diabetes Regression dataset, I recommend Partner Aâ€™s random holdout + KFold strategy as the primary evaluation method. Partner Bâ€™s ordered + time-aware approach is still valuable to understand and would be more appropriate for a true time series problem (e.g., forecasting over months or years), but it is less well-matched to this specific dataset.
