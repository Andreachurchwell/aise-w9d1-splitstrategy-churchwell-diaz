# aise-w9d1-splitstrategy-churchwell-diaz
Week 9 Split Strategy Showdown
# ğŸ§ª AISE Week 9 â€” Split Strategy Showdown  
### **Team: Andrea Churchwell & Jose Diaz**

---

## ğŸ“Œ Project Overview
This repository contains our team implementation for the **W9D1 Split Strategy Showdown** assignment.  
The goal of the project is to compare multiple evaluation strategies using the **same dataset**, the **same model**, and the **same metric**, and then analyze how each approach affects performance, variance, and overall reliability.

We are currently in the **environment setup and repository scaffolding phase**.  
Dataset selection and modeling will begin next.

---

## âœ… Current Progress

### âœ” Repository Created
- `aise-w9d1-splitstrategy-churchwell-diaz` initialized  
- `.gitignore` created  
- Project folder structure created in VS Code  

### âœ” Virtual Environment Setup
- Python virtual environment created: `venv/`  
- Core dependencies installed  
- `requirements.txt` generated via `pip freeze`  
- Git configured to ignore the virtual environment  

### âœ” Assignment Scaffolding Added
- The following required files have been created and will be populated as we progress:
- TEAM_INFO.md
- eval_partner_a.py
- eval_partner_b.py
- comparison.csv
- RECOMMENDATION.md


### âœ” README placeholder added  
Youâ€™re reading it now â€” this will grow as the project evolves.

---

## â³ Next Steps

### ğŸ”¸ **1. Confirm Dataset Choice**
We will select one of the approved datasets (e.g., Breast Cancer, Credit Card Default, Iris, etc.).  
All partners must use the *same* dataset.

### ğŸ”¸ **2. Agree on a Single Metric**
Example metrics:  
- Classification: ROC-AUC, PR-AUC, F1  
- Regression: MAE, RMSE, RÂ²  

### ğŸ”¸ **3. Populate TEAM_INFO.md**
This will include:
- Partner roles  
- Dataset choice  
- Metric choice  
- Dataset loading code  
- Package versions  

### ğŸ”¸ **4. Implement Partner A & Partner B Scripts**
- Partner A: Random 80/20 + KFold  
- Partner B: Stratified or TimeSeriesSplit + specialized 5-fold  

### ğŸ”¸ **5. Generate comparison.csv**
Both partner results will be added in a structured table.

### ğŸ”¸ **6. Write RECOMMENDATION.md**
Final analysis comparing variance, stability, leakage risk, and trustworthiness.

---

## ğŸš€ Project Status

| Step                     | Status      |
|--------------------------|------------ |
| Repo created             | âœ… Done    |
| venv + requirements.txt  | âœ… Done    |
| .gitignore added         | âœ… Done    |
| Assignment scaffolding   | âœ… Done    |
| Dataset selected         | â³ Pending |
| Metrics selected         | â³ Pending |
| Partner A code           | â³ Pending |
| Partner B code           | â³ Pending |
| comparison.csv           | â³ Pending |
| Recommendation analysis  | â³ Pending |

---

## ğŸ“ Notes

This README will continue evolving as we finalize the dataset and metric and begin implementing the required evaluation strategies.


