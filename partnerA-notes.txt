Partner A Notes â€” Andrea Churchwell
----
âœ… Dataset Details
Dataset: Diabetes Regression (load_diabetes)
Samples: 442
Features: 10 numeric
Task Type: Regression
Random State: 42
Metric: RÂ²
--------------------------------------------
This dataset tries to predict how bad someoneâ€™s diabetes is based on 10 health-related measurements.
Samples: 442

There are 442 people represented in the dataset.
This is considered small, which makes variance (score swings) very normal.

Features: 10 numeric
You have 10 input measurements (age, BMI, blood pressure, etc.).
All of the features are numbers â€” no categories, no strings â€” so preprocessing is simpler.

Task Type: Regression
This means:
You are predicting a continuous number, not a class.
The target is the disease progression score (higher = worse).

Random State: 42

This makes sure your results are repeatable.
Without a random seed, train/test split changes every time â†’ different scores.

Metric: RÂ² (Coefficient of Determination)
RÂ² answers this simple question:
â€œHow much of the variation in the target can the model explain?â€
RÂ² = 1.0 â†’ perfect prediction
RÂ² = 0.0 â†’ model predicts no better than the average
RÂ² < 0.0 â†’ model is worse than guessing
For real-world medical data like this, RÂ² around 0.45â€“0.55 is very normal.
-------------------------------------------

----

Ridge Regression is a linear model that tries to fit a straight-line relationship between the features and the target, but with a small penalty added. This penalty keeps the model from overfitting â€” meaning it prevents the model from becoming too sensitive or too â€œwigglyâ€ when the dataset is small or noisy. Because the diabetes dataset is small and has natural noise, Ridge helps keep the predictions stable and more reliable across different splits.

----

ğŸ“ Why We Scaled the Data

The 10 features in the diabetes dataset all have very different scales. Some are tiny decimals like 0.03, while others are larger. Ridge Regression is very sensitive to these differences â€” features with bigger numbers would automatically get more influence. StandardScaler fixes this by making every feature have a mean of 0 and a standard deviation of 1. This ensures all features contribute fairly. Using the scaler inside a pipeline also prevents data leakage, because the scaler only learns from the training data for each fold.

----

ğŸ“Š Train/Test Split
Train RÂ²: 0.5276

Test RÂ²: 0.4541

ğŸ‘‰ Very normal for this dataset.
ğŸ‘‰ Test is lower because itâ€™s a small dataset with high variance.Train RÂ²: 0.5276

This means:
On the training data, the model can â€œexplainâ€ 52.7% of the variation.
This is GOOD for the diabetes dataset.
It means the model is learning reasonable patterns.

Test RÂ²: 0.4541
This means:
When you test the model on new data it hasnâ€™t seen before, it explains 45.4% of the variation.
This is lower than the train score, which is normal.
The model generalizes okay â€” not great, not bad â€” exactly what we expect with a small dataset.

ğŸ‘‰ Why test RÂ² is lower
The dataset is small â†’ models struggle with generalization
Ridge regression reduces overfitting but still canâ€™t magically fix limited data
The target (disease progression) is naturally noisy
This drop is not a sign of errors.
Itâ€™s literally expected behavior.

----

----
ğŸ” 5-Fold CV Results
Fold scores:
0.46995
0.53809
0.41335
0.49030
0.49224

CV Mean: 0.4808
CV Std: 0.0404

ğŸ‘‰ CV Mean is slightly higher than test RÂ²
ğŸ‘‰ Std = 0.04 â†’ indicates mild variance across folds (normal for small datasets)
You split the training data into 5 parts (folds)
Train on 4 parts
Validate on the 5th part
Repeat 5 times

You get 5 RÂ² scores
This tests how stable the model is.
Letâ€™s break those numbers down:
Fold scores:
0.46995 â†’ solid
0.53809 â†’ highest fold, very good
0.41335 â†’ lowest fold, weaker data slice
0.49030 â†’ good
0.49224 â†’ good

What these variations mean:
The model performs a little differently depending on which slice of data it sees.
Fold #2 got a better chunk of data â†’ higher score
Fold #3 got a tough slice â†’ lower score

ğŸ’¡ This shows the dataset is sensitive to how itâ€™s split â€” VERY common for small datasets.

----

----
CV Mean: 0.4808
This is the â€œaverage performanceâ€ across all 5 folds.
It tells you:
â€œOn average, the model performs with around 48% explanatory power.â€
This is between:
training (52%)
test (45%)

ğŸ‘‰ It proves your model is consistent
ğŸ‘‰ and generalizes decently across different slices of data.
----

----
CV Std: 0.0404
This number tells you how much the scores â€œbounce around.â€

0.04 is small â†’ good
If it were 0.15+, the model would be unstable

Your model has low-to-moderate variance, which fits this dataset perfectly
Plain English:
Your model behaves mostly the same on each fold.
Thereâ€™s a little variation, but nothing crazy.
----

â˜‘ï¸ Summary
Your model learns patterns well (train RÂ² = 0.52), and it still performs reasonably on unseen data (test RÂ² = 0.45). The 5-fold CV average (0.48) sits between them, which means the model is stable and not overfitting. The folds vary a little (std = 0.04), but thatâ€™s totally normal for a small dataset like Diabetes with noisy targets. Overall, your evaluation is clean, correct, and exactly what the assignment expects for Partner A.

----

----

ğŸ”’ Data Leakage Notes (Partner A)

Random 80/20 splitting is safe for this dataset because there is no time component and no repeated patients. However, leakage could happen if scaling was done on the full dataset before splitting. Using a pipeline prevents this because StandardScaler fits only on the training split and inside each CV fold. This ensures no information from the test set leaks into the model.

----


Partner B Notes â€” JosÃ© Diaz
(Ordered Holdout + Time-Aware CV)
âœ… Dataset Details

Dataset: Diabetes Regression (load_diabetes)
Samples: 442
Features: 10 numeric
Task Type: Regression
Metric: RÂ²
Random State: Not applicable for Partner B since the split is ordered (no randomness)

ğŸ“Œ What This Dataset Represents
Same as Partner A â€” both partners must use identical data.
The diabetes dataset predicts a continuous progression score based on 10 medical features like BMI, blood pressure, and age. Itâ€™s small and noisy, which makes evaluation very sensitive to how the data is split. That sensitivity explains why Partner Bâ€™s folds bounce around more than Partner Aâ€™s.

--------------------------------------------
ğŸŸ¦ Partner B Strategy Overview

Partner Bâ€™s job is to simulate a time-aware evaluation pipeline:

âœ” Ordered Train/Test Holdout (first 80% â†’ train, last 20% â†’ test)
âœ” Time-Aware Cross Validation using TimeSeriesSplit
âœ” Same model (Ridge Regression) and same scaler (StandardScaler in a Pipeline)
âœ” Same metric (RÂ²)

Even though the dataset is not truly time series, the assignment requires Partner B to practice a â€œfuture-awareâ€ strategy to compare its behavior against random shuffle.

This creates real, noticeable differences in score stability.

--------------------------------------------

ğŸŸ¦ Why Ordered 80/20 Holdout?

Partner B uses:
X_train = first 80% of rows  
X_test = last 20% of rows

No shuffling. No randomness.
This simulates â€œtrain on the past â†’ predict the future.â€

This matters because:
If the dataset had seasonality or trending patterns, this avoids leakage.
But this dataset does not have time patterns â†’ so the split becomes â€œharsherâ€ for no real benefit.
The model gets stuck with whatever patterns appear in the last 20% of the dataset.
ğŸ‘‰ Result: More score volatility and less stability.

--------------------------------------------
ğŸŸ¦ Model: Ridge Regression Pipeline

Same as Partner A.
Ridge regression is chosen because:
The dataset is noisy
Features are correlated
Ridge reduces overfitting
The pipeline ensures scaling happens safely per fold (no leakage)

--------------------------------------------
ğŸŸ¦ Train/Test Performance
The outputs JosÃ© printed:
Train RÂ²: 0.5088
Test RÂ²: 0.5413

ğŸ§  How To Interpret This:
His train RÂ² (0.5088) is very similar to yours (0.5276)
His test RÂ² (0.5413) is higher than his train score
Thatâ€™s unusual but not an error â€” it can happen in small datasets
Why?
Because:
The final 20% of the dataset (his test slice) may be easier than earlier rows
Ordered splits can accidentally create â€œluckyâ€ test sets or â€œharshâ€ train sets

No randomness means the model isnâ€™t statistically balanced
ğŸ‘‰ His test score being higher doesnâ€™t mean his method is superior.
It means the last rows happened to match the modelâ€™s learned patterns better.

--------------------------------------------

ğŸŸ¦ TimeSeriesSplit â€“ 5-Fold Time-Aware CV
Partner B uses:
TimeSeriesSplit(n_splits=5)
This creates folds where:
The training portion always comes first
The validation portion is always later
More realistic for true temporal prediction

Much more unstable for datasets with no time dimension

ğŸ“Š Partner B Fold Scores:
0.2984
0.5760
0.2785
0.5831
0.4492

âœ” CV Mean: 0.4373
âœ” CV Std: 0.1305

ğŸ§  How To Interpret These Numbers
ğŸ“‰ Huge variability
Lowest fold: 0.27
Highest fold: 0.58
Std Dev = 0.13 â†’ large swings

This tells you:
The dataset behaves very differently depending on which â€œtime chunkâ€ it sees
Since the dataset isnâ€™t temporal, these swings come from noise

TimeSeriesSplit pushes the model into unstable territory unnecessarily
ğŸ“Œ This is the expected behavior of TimeSeriesSplit on a non-time dataset.
Itâ€™s not a mistake. Itâ€™s simply less appropriate for this dataset.

--------------------------------------------
ğŸŸ¦ Understanding Why Partner B Performs This Way

Partner Bâ€™s method:
âœ” Avoids any chance of temporal leakage
âœ” Is better for forecasting-style datasets
âœ– Is less reliable for small, non-time datasets
âœ– Produces higher variance
âœ– Often gives lower average performance

In your comparison:
Partner Bâ€™s strategy is more â€œreal-world time-aware,â€
but less appropriate for the Diabetes dataset â†’ which has no temporal structure.

--------------------------------------------
ğŸŸ¦ Visualizations 
JosÃ© generates three Plotly visuals:
5-Fold Time-Aware CV Bar Chart

Shows the dramatic differences between folds
Actual vs Predicted Scatter (Test Set)

Tests the relationship between predicted and true values
Residual Histogram
Helps visualize prediction error distribution

These visuals are correct, helpful, and match the assignment's â€œstorytellingâ€ suggestions.

--------------------------------------------
ğŸŸ¦ Summary 

Partner Bâ€™s ordered holdout and TimeSeriesSplit approach produced a mean RÂ² of 0.4373 with high fold-to-fold variance (std = 0.1305). This evaluation strategy simulates realistic forecasting conditions, where models train on earlier data and predict future observations. However, since the diabetes dataset is not temporal, this method introduces unnecessary instability. Partner Bâ€™s test RÂ² (0.5413) was slightly higher than the train RÂ² (0.5088), likely due to the final 20% of the dataset being easier for the model. Overall, Partner Bâ€™s approach is correct for time-aware evaluation, but less appropriate for this particular dataset compared to Partner Aâ€™s randomized approach.

--------------------------------------------
