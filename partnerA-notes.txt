Partner A Notes â€” Andrea Churchwell
----
âœ… Dataset Details
Dataset: Diabetes Regression (load_diabetes)
Samples: 442
Features: 10 numeric
Task Type: Regression
Random State: 42
Metric: RÂ²
--------------------------------------------
This dataset tries to predict how bad someoneâ€™s diabetes is based on 10 health-related measurements.
Samples: 442

There are 442 people represented in the dataset.
This is considered small, which makes variance (score swings) very normal.

Features: 10 numeric
You have 10 input measurements (age, BMI, blood pressure, etc.).
All of the features are numbers â€” no categories, no strings â€” so preprocessing is simpler.

Task Type: Regression
This means:
You are predicting a continuous number, not a class.
The target is the disease progression score (higher = worse).

Random State: 42

This makes sure your results are repeatable.
Without a random seed, train/test split changes every time â†’ different scores.

Metric: RÂ² (Coefficient of Determination)
RÂ² answers this simple question:
â€œHow much of the variation in the target can the model explain?â€
RÂ² = 1.0 â†’ perfect prediction
RÂ² = 0.0 â†’ model predicts no better than the average
RÂ² < 0.0 â†’ model is worse than guessing
For real-world medical data like this, RÂ² around 0.45â€“0.55 is very normal.
-------------------------------------------

----

Ridge Regression is a linear model that tries to fit a straight-line relationship between the features and the target, but with a small penalty added. This penalty keeps the model from overfitting â€” meaning it prevents the model from becoming too sensitive or too â€œwigglyâ€ when the dataset is small or noisy. Because the diabetes dataset is small and has natural noise, Ridge helps keep the predictions stable and more reliable across different splits.

----

ğŸ“ Why We Scaled the Data

The 10 features in the diabetes dataset all have very different scales. Some are tiny decimals like 0.03, while others are larger. Ridge Regression is very sensitive to these differences â€” features with bigger numbers would automatically get more influence. StandardScaler fixes this by making every feature have a mean of 0 and a standard deviation of 1. This ensures all features contribute fairly. Using the scaler inside a pipeline also prevents data leakage, because the scaler only learns from the training data for each fold.

----

ğŸ“Š Train/Test Split
Train RÂ²: 0.5276

Test RÂ²: 0.4541

ğŸ‘‰ Very normal for this dataset.
ğŸ‘‰ Test is lower because itâ€™s a small dataset with high variance.Train RÂ²: 0.5276

This means:
On the training data, the model can â€œexplainâ€ 52.7% of the variation.
This is GOOD for the diabetes dataset.
It means the model is learning reasonable patterns.

Test RÂ²: 0.4541
This means:
When you test the model on new data it hasnâ€™t seen before, it explains 45.4% of the variation.
This is lower than the train score, which is normal.
The model generalizes okay â€” not great, not bad â€” exactly what we expect with a small dataset.

ğŸ‘‰ Why test RÂ² is lower
The dataset is small â†’ models struggle with generalization
Ridge regression reduces overfitting but still canâ€™t magically fix limited data
The target (disease progression) is naturally noisy
This drop is not a sign of errors.
Itâ€™s literally expected behavior.

----

----
ğŸ” 5-Fold CV Results
Fold scores:
0.46995
0.53809
0.41335
0.49030
0.49224

CV Mean: 0.4808
CV Std: 0.0404

ğŸ‘‰ CV Mean is slightly higher than test RÂ²
ğŸ‘‰ Std = 0.04 â†’ indicates mild variance across folds (normal for small datasets)
You split the training data into 5 parts (folds)
Train on 4 parts
Validate on the 5th part
Repeat 5 times

You get 5 RÂ² scores
This tests how stable the model is.
Letâ€™s break those numbers down:
Fold scores:
0.46995 â†’ solid
0.53809 â†’ highest fold, very good
0.41335 â†’ lowest fold, weaker data slice
0.49030 â†’ good
0.49224 â†’ good

What these variations mean:
The model performs a little differently depending on which slice of data it sees.
Fold #2 got a better chunk of data â†’ higher score
Fold #3 got a tough slice â†’ lower score

ğŸ’¡ This shows the dataset is sensitive to how itâ€™s split â€” VERY common for small datasets.

----

----
CV Mean: 0.4808
This is the â€œaverage performanceâ€ across all 5 folds.
It tells you:
â€œOn average, the model performs with around 48% explanatory power.â€
This is between:
training (52%)
test (45%)

ğŸ‘‰ It proves your model is consistent
ğŸ‘‰ and generalizes decently across different slices of data.
----

----
CV Std: 0.0404
This number tells you how much the scores â€œbounce around.â€

0.04 is small â†’ good
If it were 0.15+, the model would be unstable

Your model has low-to-moderate variance, which fits this dataset perfectly
Plain English:
Your model behaves mostly the same on each fold.
Thereâ€™s a little variation, but nothing crazy.
----

â˜‘ï¸ Summary
Your model learns patterns well (train RÂ² = 0.52), and it still performs reasonably on unseen data (test RÂ² = 0.45). The 5-fold CV average (0.48) sits between them, which means the model is stable and not overfitting. The folds vary a little (std = 0.04), but thatâ€™s totally normal for a small dataset like Diabetes with noisy targets. Overall, your evaluation is clean, correct, and exactly what the assignment expects for Partner A.

----

----

ğŸ”’ Data Leakage Notes (Partner A)

Random 80/20 splitting is safe for this dataset because there is no time component and no repeated patients. However, leakage could happen if scaling was done on the full dataset before splitting. Using a pipeline prevents this because StandardScaler fits only on the training split and inside each CV fold. This ensures no information from the test set leaks into the model.

----


